{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46901e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode, create_react_agent\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from typing import Literal, TypedDict, Annotated,List\n",
    "import operator\n",
    "import os\n",
    "from typing import Literal\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d59dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class QualityScore(BaseModel):\n",
    "    clarity: int = Field(ge=1, le=5)\n",
    "    completeness: int = Field(ge=1, le=5)\n",
    "    accuracy: int = Field(ge=1, le=5)\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "719656c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectionState(TypedDict):\n",
    "    task: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    scores: QualityScore | None\n",
    "    score_history: List[QualityScore]\n",
    "    iterations: int\n",
    "    final_output: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fca0099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key loaded\n"
     ]
    }
   ],
   "source": [
    "# Load API key\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"openai_key\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found!\")\n",
    "\n",
    "print(\"‚úÖ API key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "699772d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ LLM initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10c8d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generator(state: ReflectionState) -> dict:\n",
    "    \"\"\"Generate or refine based on critique.\"\"\"\n",
    "    if state[\"iterations\"] == 0:\n",
    "        prompt = f\"\"\"\n",
    "Task: {state['task']}\n",
    "\n",
    "Create a clear, complete, and accurate response.\"\"\"\n",
    "        print(\"\\n‚úçÔ∏è Generating initial draft...\")\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "        Task: {state['task']}\n",
    "\n",
    "        Current Draft:\n",
    "        {state['draft']}\n",
    "\n",
    "        Critic Feedback:\n",
    "        {state['critique']}\n",
    "\n",
    "        Improve the draft addressing all weaknesses.\n",
    "        \"\"\"\n",
    "        print(f\"\\n‚úçÔ∏è Refining (iteration {state['iterations']})...\")\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    return {\"draft\": response.content}\n",
    "\n",
    "\n",
    "def critic(state: ReflectionState) -> dict:\n",
    "    prompt = f\"\"\"\n",
    "    Evaluate the response below using a 1-5 scale.\n",
    "\n",
    "    Task:\n",
    "    {state['task']}\n",
    "\n",
    "    Response:\n",
    "    {state['draft']}\n",
    "\n",
    "    Score strictly using this JSON format:\n",
    "    {{\n",
    "      \"clarity\": int,\n",
    "      \"completeness\": int,\n",
    "      \"accuracy\": int,\n",
    "      \"feedback\": \"text\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Scoring draft...\")\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    raw = response.content.strip()\n",
    "\n",
    "    scores = QualityScore(**json.loads(raw))\n",
    "\n",
    "    history = state.get(\"score_history\", [])\n",
    "    history.append(scores)\n",
    "\n",
    "    print(\n",
    "        f\"Iteration {state['iterations'] + 1}: \"\n",
    "        f\"Clarity={scores.clarity}, \"\n",
    "        f\"Completeness={scores.completeness}, \"\n",
    "        f\"Accuracy={scores.accuracy}\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"scores\": scores,\n",
    "        \"critique\": scores.feedback,\n",
    "        \"score_history\": history,\n",
    "        \"iterations\": state[\"iterations\"] + 1,\n",
    "    }\n",
    "\n",
    "def reflection_finalizer(state: ReflectionState) -> dict:\n",
    "    return {\n",
    "        \"final_output\": state[\"draft\"]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "MAX_REFLECTIONS = 3\n",
    "\n",
    "def should_reflect_again(state: ReflectionState) -> Literal[\"generator\", \"reflection_finalizer\"]:\n",
    "\n",
    "    scores = state[\"scores\"]\n",
    "\n",
    "    if (scores.clarity >= 4 and scores.completeness >= 4 and scores.accuracy >= 4):\n",
    "        print(\"‚úÖ All quality thresholds met\")\n",
    "        return \"finalizer\"\n",
    "\n",
    "    if state[\"iterations\"] >= MAX_REFLECTIONS:\n",
    "        print(\"‚ö†Ô∏è Max iterations reached\")\n",
    "        return \"finalizer\"\n",
    "\n",
    "    print(\"üîÅ Quality below threshold ‚Üí refining\")\n",
    "    return \"generator\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74eb4e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "builder = StateGraph(ReflectionState)\n",
    "\n",
    "builder.add_node(\"generator\", generator)\n",
    "builder.add_node(\"critic\", critic)\n",
    "builder.add_node(\"finalizer\", reflection_finalizer)\n",
    "\n",
    "builder.add_edge(START, \"generator\")\n",
    "builder.add_edge(\"generator\", \"critic\")\n",
    "builder.add_conditional_edges(\n",
    "    \"critic\",\n",
    "    should_reflect_again,\n",
    "    {\n",
    "        \"generator\": \"generator\",\n",
    "        \"finalizer\": \"finalizer\"\n",
    "    }\n",
    ")\n",
    "builder.add_edge(\"finalizer\", END)\n",
    "\n",
    "reflection_agent = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be2003de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úçÔ∏è Generating initial draft...\n",
      "Scoring draft...\n",
      "Iteration 1: Clarity=5, Completeness=5, Accuracy=5\n",
      "‚úÖ All quality thresholds met\n"
     ]
    }
   ],
   "source": [
    "result = reflection_agent.invoke({\n",
    "    \"task\": \"Explain how RAG works in large language models.\",\n",
    "    \"draft\": \"\",\n",
    "    \"critique\": \"\",\n",
    "    \"scores\": None,\n",
    "    \"score_history\": [],\n",
    "    \"iterations\": 0,\n",
    "    \"final_output\": \"\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "444fb244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Clarity=5, Completeness=5, Accuracy=5\n"
     ]
    }
   ],
   "source": [
    "for i, s in enumerate(result[\"score_history\"], 1):\n",
    "    print(\n",
    "        f\"Iteration {i}: \"\n",
    "        f\"Clarity={s.clarity}, \"\n",
    "        f\"Completeness={s.completeness}, \"\n",
    "        f\"Accuracy={s.accuracy}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
