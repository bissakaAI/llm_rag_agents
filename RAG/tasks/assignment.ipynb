{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f76922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Knowledge Base\n",
    "# -----------------------------\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain is a framework for building applications using large language models.\"),\n",
    "    Document(page_content=\"Retrieval-Augmented Generation (RAG) improves LLM responses by retrieving relevant context.\"),\n",
    "    Document(page_content=\"LangChain Expression Language allows declarative construction of LLM pipelines.\"),\n",
    "    Document(page_content=\"Chunking strategy directly affects retrieval accuracy in RAG systems.\"),\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Chunk Documents\n",
    "# -----------------------------\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=30\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Vector Store & Retriever\n",
    "# -----------------------------\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(splits, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Chat History Store\n",
    "# -----------------------------\n",
    "chat_store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in chat_store:\n",
    "        chat_store[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_store[session_id]\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Prompt Template\n",
    "# -----------------------------\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question using ONLY the provided context.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion:\\n{question}\")\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# 6. LLM\n",
    "# -----------------------------\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Helper: Format Retrieved Docs\n",
    "# -----------------------------\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Build RAG Chain (LCEL)\n",
    "# -----------------------------\n",
    "rag_chain_base = (\n",
    "    RunnableParallel(\n",
    "        context=lambda x: format_docs(retriever.invoke(x[\"question\"])),\n",
    "        question=lambda x: x[\"question\"],\n",
    "        chat_history=lambda x: x.get(\"chat_history\", [])\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Add Conversational Memory\n",
    "# -----------------------------\n",
    "rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain_base,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Run the System\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    session_id = \"student-session\"\n",
    "\n",
    "    print(\"Q1: What is RAG?\")\n",
    "    answer1 = rag_chain.invoke(\n",
    "        {\"question\": \"What is RAG?\"},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    print(\"A1:\", answer1, \"\\n\")\n",
    "\n",
    "    print(\"Q2: Why is chunking important?\")\n",
    "    answer2 = rag_chain.invoke(\n",
    "        {\"question\": \"Why is chunking important in RAG systems?\"},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    print(\"A2:\", answer2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
