{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44c3289d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length: 1367 characters\n",
      "Document length: 187 words\n",
      "\n",
      "First 200 characters:\n",
      "\n",
      "Python is a high-level, interpreted programming language known for its simplicity and readability. \n",
      "It was created by Guido van Rossum and first released in 1991. Python supports multiple programming...\n"
     ]
    }
   ],
   "source": [
    "# Sample document about Python programming\n",
    "sample_document = \"\"\"\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability. \n",
    "It was created by Guido van Rossum and first released in 1991. Python supports multiple programming \n",
    "paradigms including procedural, object-oriented, and functional programming.\n",
    "\n",
    "One of Python's key strengths is its extensive standard library, which provides tools for many common \n",
    "programming tasks. The language emphasizes code readability with its use of significant indentation. \n",
    "Python's syntax allows programmers to express concepts in fewer lines of code compared to languages \n",
    "like C++ or Java.\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, scientific computing, \n",
    "and automation. Popular frameworks include Django and Flask for web development, NumPy and Pandas for \n",
    "data analysis, and TensorFlow and PyTorch for machine learning.\n",
    "\n",
    "The Python Package Index (PyPI) hosts hundreds of thousands of third-party packages that extend Python's \n",
    "capabilities. Installation is simple using pip, Python's package installer. The active community contributes \n",
    "to a rich ecosystem of libraries and frameworks.\n",
    "\n",
    "Python continues to be one of the most popular programming languages worldwide. Its beginner-friendly nature \n",
    "makes it ideal for education, while its powerful features support professional software development and research.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Document length: {len(sample_document)} characters\")\n",
    "print(f\"Document length: {len(sample_document.split())} words\")\n",
    "print(f\"\\nFirst 200 characters:\\n{sample_document[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d1c94bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 10\n",
      "\n",
      "Chunk 1 (200 chars):\n",
      "\n",
      "Python is a high-level, interpreted programming language known for its simplicity and readability. \n",
      "It was created by Guido van Rossum and first released in 1991. Python supports multiple programming\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2 (200 chars):\n",
      "ased in 1991. Python supports multiple programming \n",
      "paradigms including procedural, object-oriented, and functional programming.\n",
      "\n",
      "One of Python's key strengths is its extensive standard library, which\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3 (200 chars):\n",
      "strengths is its extensive standard library, which provides tools for many common \n",
      "programming tasks. The language emphasizes code readability with its use of significant indentation. \n",
      "Python's syntax\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def chunk_by_characters(text, chunk_size=200, overlap=50):\n",
    "    \"\"\"\n",
    "    Split text into chunks of specified character length.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to chunk\n",
    "        chunk_size: Number of characters per chunk\n",
    "        overlap: Number of characters to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        # Get chunk from start to start + chunk_size\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        # Move start position (with overlap)\n",
    "        start += chunk_size - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test it\n",
    "chunks = chunk_by_characters(sample_document, chunk_size=200, overlap=50)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\\n\")\n",
    "for i, chunk in enumerate(chunks[:3], 1):  # Show first 3 chunks\n",
    "    print(f\"Chunk {i} ({len(chunk)} chars):\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19019d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 5\n",
      "\n",
      "Chunk 1 (50 words):\n",
      "Python is a high-level, interpreted programming language known for its simplicity and readability. It was created by Guido van Rossum and first released in 1991. Python supports multiple programming paradigms including procedural, object-oriented, and functional programming. One of Python's key strengths is its extensive standard library, which provides tools for\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2 (50 words):\n",
      "strengths is its extensive standard library, which provides tools for many common programming tasks. The language emphasizes code readability with its use of significant indentation. Python's syntax allows programmers to express concepts in fewer lines of code compared to languages like C++ or Java. Python is widely used in web\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3 (50 words):\n",
      "like C++ or Java. Python is widely used in web development, data science, artificial intelligence, scientific computing, and automation. Popular frameworks include Django and Flask for web development, NumPy and Pandas for data analysis, and TensorFlow and PyTorch for machine learning. The Python Package Index (PyPI) hosts hundreds of thousands\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def chunk_by_words(text, chunk_size=50, overlap=10):\n",
    "    \"\"\"\n",
    "    Split text into chunks of specified word count.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to chunk\n",
    "        chunk_size: Number of words per chunk\n",
    "        overlap: Number of words to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(words):\n",
    "        # Get chunk of words\n",
    "        end = start + chunk_size\n",
    "        chunk_words = words[start:end]\n",
    "        \n",
    "        # Join words back into text\n",
    "        chunk = ' '.join(chunk_words)\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        # Move start position (with overlap)\n",
    "        start += chunk_size - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test it\n",
    "chunks = chunk_by_words(sample_document, chunk_size=50, overlap=10)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\\n\")\n",
    "for i, chunk in enumerate(chunks[:3], 1):  # Show first 3 chunks\n",
    "    print(f\"Chunk {i} ({len(chunk.split())} words):\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6ab4dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 4\n",
      "\n",
      "Chunk 1 (398 chars):\n",
      "Python is a high-level, interpreted programming language known for its simplicity and readability. It was created by Guido van Rossum and first released in 1991. Python supports multiple programming \n",
      "paradigms including procedural, object-oriented, and functional programming. One of Python's key strengths is its extensive standard library, which provides tools for many common \n",
      "programming tasks.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2 (320 chars):\n",
      "The language emphasizes code readability with its use of significant indentation. Python's syntax allows programmers to express concepts in fewer lines of code compared to languages \n",
      "like C++ or Java. Python is widely used in web development, data science, artificial intelligence, scientific computing, \n",
      "and automation.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3 (332 chars):\n",
      "Popular frameworks include Django and Flask for web development, NumPy and Pandas for \n",
      "data analysis, and TensorFlow and PyTorch for machine learning. The Python Package Index (PyPI) hosts hundreds of thousands of third-party packages that extend Python's \n",
      "capabilities. Installation is simple using pip, Python's package installer.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 4 (306 chars):\n",
      "The active community contributes \n",
      "to a rich ecosystem of libraries and frameworks. Python continues to be one of the most popular programming languages worldwide. Its beginner-friendly nature \n",
      "makes it ideal for education, while its powerful features support professional software development and research.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def chunk_by_sentences(text, max_chunk_size=500):\n",
    "    \"\"\"\n",
    "    Split text into chunks by sentences, keeping sentences intact.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to chunk\n",
    "        max_chunk_size: Maximum characters per chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (split on . ! ?)\n",
    "    import re\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Check if adding this sentence would exceed max size\n",
    "        if len(current_chunk) + len(sentence) > max_chunk_size and current_chunk:\n",
    "            # Save current chunk and start new one\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            # Add sentence to current chunk\n",
    "            current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "    \n",
    "    # Don't forget the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test it\n",
    "chunks = chunk_by_sentences(sample_document, max_chunk_size=400)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i} ({len(chunk)} chars):\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bc7db24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 5\n",
      "\n",
      "Chunk 1 (277 chars):\n",
      "Python is a high-level, interpreted programming language known for its simplicity and readability. \n",
      "It was created by Guido van Rossum and first released in 1991. Python supports multiple programming \n",
      "paradigms including procedural, object-oriented, and functional programming.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2 (323 chars):\n",
      "One of Python's key strengths is its extensive standard library, which provides tools for many common \n",
      "programming tasks. The language emphasizes code readability with its use of significant indentation. \n",
      "Python's syntax allows programmers to express concepts in fewer lines of code compared to languages \n",
      "like C++ or Java.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3 (270 chars):\n",
      "Python is widely used in web development, data science, artificial intelligence, scientific computing, \n",
      "and automation. Popular frameworks include Django and Flask for web development, NumPy and Pandas for \n",
      "data analysis, and TensorFlow and PyTorch for machine learning.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 4 (264 chars):\n",
      "The Python Package Index (PyPI) hosts hundreds of thousands of third-party packages that extend Python's \n",
      "capabilities. Installation is simple using pip, Python's package installer. The active community contributes \n",
      "to a rich ecosystem of libraries and frameworks.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 5 (223 chars):\n",
      "Python continues to be one of the most popular programming languages worldwide. Its beginner-friendly nature \n",
      "makes it ideal for education, while its powerful features support professional software development and research.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def chunk_by_paragraphs(text, min_chunk_size=100):\n",
    "    \"\"\"\n",
    "    Split text by paragraphs (double newlines).\n",
    "    \n",
    "    Args:\n",
    "        text: The text to chunk\n",
    "        min_chunk_size: Minimum characters per chunk (combine small paragraphs)\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Split by double newlines (paragraph separator)\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        para = para.strip()\n",
    "        if not para:\n",
    "            continue\n",
    "            \n",
    "        # If paragraph is too small, combine with next\n",
    "        if len(para) < min_chunk_size:\n",
    "            current_chunk += \"\\n\\n\" + para if current_chunk else para\n",
    "        else:\n",
    "            # Save previous chunk if exists\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            # Start new chunk with this paragraph\n",
    "            current_chunk = para\n",
    "    \n",
    "    # Don't forget the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test it\n",
    "chunks = chunk_by_paragraphs(sample_document, min_chunk_size=100)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i} ({len(chunk)} chars):\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ecb9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 5\n",
      "\n",
      "First chunk with metadata:\n",
      "================================================================================\n",
      "Text: strengths is its extensive standard library, which provides tools for many common programming tasks. The language emphasizes code readability with its use of significant indentation. Python's syntax a...\n",
      "\n",
      "Metadata:\n",
      "  source: python_intro.txt\n",
      "  chunk_index: 1\n",
      "  chunk_size: 50\n",
      "  char_count: 329\n",
      "  start_word: 40\n",
      "  end_word: 90\n"
     ]
    }
   ],
   "source": [
    "def chunk_with_metadata(text, source_name, chunk_size=50, overlap=10):\n",
    "    \"\"\"\n",
    "    Create chunks with metadata.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to chunk\n",
    "        source_name: Name of the source document\n",
    "        chunk_size: Number of words per chunk\n",
    "        overlap: Number of words to overlap\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with 'text' and 'metadata'\n",
    "    \"\"\"\n",
    "\n",
    "    # Get basic chunks \n",
    "    words= text.split()\n",
    "    chunks=[]\n",
    "    start=0\n",
    "    chunk_index=0\n",
    "\n",
    "    while start<len(words):\n",
    "        end=start+chunk_size\n",
    "        chunk_words=words[start:end]\n",
    "        chunk_text= ' '.join(chunk_words)\n",
    "\n",
    "        #create chunk with metadata\n",
    "        chunk_with_meta ={\n",
    "            'text':chunk_text,\n",
    "            'metadata':{'source':source_name,\n",
    "                        'chunk_index':chunk_index,\n",
    "                        'chunk_size':len(chunk_words),\n",
    "                        'char_count':len(chunk_text),\n",
    "                        'start_word':start,\n",
    "                        'end_word':end\n",
    "                        }\n",
    "        }\n",
    "        chunks.append(chunk_with_meta)\n",
    "        start += chunk_size - overlap\n",
    "        chunk_index +=1\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Test it \n",
    "chunks= chunk_with_metadata(sample_document,source_name='python_intro.txt',chunk_size=50,overlap=10)\n",
    "print(f\"Total chunks: {len(chunks)}\\n\")\n",
    "print(\"First chunk with metadata:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Text: {chunks[1]['text'][:200]}...\")\n",
    "print(f\"\\nMetadata:\")\n",
    "for key, value in chunks[1]['metadata'].items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30948f5c",
   "metadata": {},
   "source": [
    "6.1.  Loading Text Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d5a103a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and chunked: sample_document.txt\n",
      "Total chunks: 4\n",
      "\n",
      "Chunk 1:\n",
      "Python is a high-level, interpreted programming language known for its simplicity and readability. It was created by Guido van Rossum and first released in 1991. Python supports multiple programming \n",
      "paradigms including procedural, object-oriented, and functional programming. One of Python's key strengths is its extensive standard library, which provides tools for many common \n",
      "programming tasks.\n"
     ]
    }
   ],
   "source": [
    "def load_and_chunk_text_file(file_path, chunk_size=500, overlap=50):\n",
    "    \"\"\"\n",
    "    Load a text file and chunk it.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the text file\n",
    "        chunk_size: Characters per chunk\n",
    "        overlap: Character overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of chunks with metadata\n",
    "    \"\"\"\n",
    "    import os \n",
    "\n",
    "    #Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text= f.read()\n",
    "\n",
    "    #get file metadata\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_size= os.path.getsize(file_path)\n",
    "\n",
    "    # chunk the text\n",
    "    chunks= chunk_by_sentences(text,max_chunk_size=chunk_size)\n",
    "\n",
    "    #Add metadata to each chunk \n",
    "    chunks_with_metadata =[]\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunks_with_metadata.append({\n",
    "            'text': chunk,\n",
    "            'metadata': {\n",
    "                'source': file_name,\n",
    "                'file_path': file_path,\n",
    "                'file_size': file_size,\n",
    "                'chunk_index': i,\n",
    "                'total_chunks': len(chunks)\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return chunks_with_metadata\n",
    "\n",
    "# Example usage (create a sample file first)\n",
    "sample_file_path = 'sample_document.txt'\n",
    "with open(sample_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_document)\n",
    "\n",
    "# Load and chunk\n",
    "chunks = load_and_chunk_text_file(sample_file_path, chunk_size=400)\n",
    "\n",
    "print(f\"Loaded and chunked: {chunks[0]['metadata']['source']}\")\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(f\"\\nChunk 1:\")\n",
    "print(chunks[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc014fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'sample_document.txt',\n",
       " 'file_path': 'sample_document.txt',\n",
       " 'file_size': 1387,\n",
       " 'chunk_index': 2,\n",
       " 'total_chunks': 4}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[2]['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78d7511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8aecc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded!\n",
      "Model produces 384 dimensional embeddings\n"
     ]
    }
   ],
   "source": [
    "# Load a small, fast embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"✅ Model loaded!\")\n",
    "print(f\"Model produces {model.get_sentence_embedding_dimension()} dimensional embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995a4d01",
   "metadata": {},
   "source": [
    "Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e353c518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The cat sat on the mat\n",
      "Embedding shape: (384,)\n",
      "Embedding type: <class 'numpy.ndarray'>\n",
      "\n",
      "First 10 values: [ 0.13040186 -0.01187012 -0.02811704  0.05123863 -0.05597441  0.03019154\n",
      "  0.03016129  0.02469839 -0.01837056  0.05876678]\n"
     ]
    }
   ],
   "source": [
    "text= \"The cat sat on the mat\"\n",
    "\n",
    "embedding= model.encode(text)\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding type: {type(embedding)}\")\n",
    "print(f\"\\nFirst 10 values: {embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f170e65",
   "metadata": {},
   "source": [
    "cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "045d777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity function ready!\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1,vec2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    Return a score between -1 -1 (higher= more similar)\n",
    "    \"\"\"\n",
    "    dot_product= np.dot(vec1,vec2)\n",
    "    norm1= np.linalg.norm(vec1)\n",
    "    norm2= np.linalg.norm(vec2)\n",
    "    return dot_product/(norm1*norm2)\n",
    "\n",
    "print(\"Similarity function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78acc06",
   "metadata": {},
   "source": [
    "### Testing Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b0915a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing to: 'The cat sat on the mat'\n",
      "\n",
      "Similarity to 'The cat sat on the mat'\n",
      "Score: 1.000\n",
      "\n",
      "Similarity to 'A feline rested on the rug'\n",
      "Score: 0.564\n",
      "\n",
      "Similarity to 'Dogs are loyal animals'\n",
      "Score: 0.165\n",
      "\n",
      "Similarity to 'Python is a programming language'\n",
      "Score: 0.031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create test sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"A feline rested on the rug\",      # Similar meaning, different words\n",
    "    \"Dogs are loyal animals\",          # Different topic\n",
    "    \"Python is a programming language\" # Completely unrelated\n",
    "]\n",
    "\n",
    "\n",
    "# generate embeddings for all sentenses\n",
    "embeddings= model.encode(sentences)\n",
    "\n",
    "#compare first sentence to all others \n",
    "print(\"Comparing to: 'The cat sat on the mat'\\n\")\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    similarity= cosine_similarity(embeddings[0],embeddings[i])\n",
    "    print(f\"Similarity to '{sentence}'\")\n",
    "    print(f\"Score: {similarity:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe69f83",
   "metadata": {},
   "source": [
    "## Building a Simple Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3724ab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base: 8 documents\n"
     ]
    }
   ],
   "source": [
    "# Sample knowledge base\n",
    "documents = [\n",
    "    \"Python is a high-level programming language known for simplicity\",\n",
    "    \"Machine learning enables computers to learn from data\",\n",
    "    \"Neural networks are inspired by biological brains\",\n",
    "    \"Dogs are loyal and friendly pets that need exercise\",\n",
    "    \"Cats are independent animals that make great companions\",\n",
    "    \"JavaScript is used for web development and runs in browsers\",\n",
    "    \"Deep learning uses multi-layered neural networks\",\n",
    "    \"Puppies require training and socialization from an early age\"\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base: {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583cbd9b",
   "metadata": {},
   "source": [
    "## Embed All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00adc485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for ALL DOCUMENTS...\n",
      "✅ Created 8 embeddings\n",
      "Each embedding has 384 dimensions\n"
     ]
    }
   ],
   "source": [
    "#Generate embeddings for all documents \n",
    "print(\"Generating embeddings for ALL DOCUMENTS...\")\n",
    "doc_embeddings = model.encode(documents)\n",
    "\n",
    "print(f\"✅ Created {len(doc_embeddings)} embeddings\")\n",
    "print(f\"Each embedding has {doc_embeddings[0].shape[0]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643d2282",
   "metadata": {},
   "source": [
    "## Search Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cae03ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Search function ready!\n"
     ]
    }
   ],
   "source": [
    "def search(query,documents,doc_embeddings,top_k=3):\n",
    "    \"\"\" Search for documents similar to the query.\n",
    "    \n",
    "    Args:\n",
    "    query:Search query (string)\n",
    "    documents:List of document texts\n",
    "    doc_embeddings: Pre-computed documents embeddings\n",
    "    top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of (document, similarity_score) tuples\n",
    "    \"\"\"\n",
    "\n",
    "    #Embed the query\n",
    "    query_embedding= model.encode(query)\n",
    "\n",
    "    #calculate similarities\n",
    "    similarities=[]\n",
    "    for i,doc_emb in enumerate (doc_embeddings):\n",
    "        similarity = cosine_similarity(query_embedding,doc_emb)\n",
    "        similarities.append((documents[i],similarity))\n",
    "\n",
    "    #sorting by similarity (highest first)\n",
    "\n",
    "    similarities.sort(key=lambda x:x[1],reverse=True)\n",
    "\n",
    "    #return top k results\n",
    "    return similarities[:top_k]\n",
    "print(\"✅ Search function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d6f18",
   "metadata": {},
   "source": [
    "## Try it out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36a5a36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUERY: What is artificial intelligence?\n",
      "================================================================================\n",
      "\n",
      "1. (Score: 0.408)\n",
      "   Machine learning enables computers to learn from data\n",
      "\n",
      "2. (Score: 0.395)\n",
      "   Neural networks are inspired by biological brains\n",
      "\n",
      "3. (Score: 0.326)\n",
      "   Python is a high-level programming language known for simplicity\n",
      "\n",
      "================================================================================\n",
      "QUERY: Tell me about pet dogs\n",
      "================================================================================\n",
      "\n",
      "1. (Score: 0.548)\n",
      "   Dogs are loyal and friendly pets that need exercise\n",
      "\n",
      "2. (Score: 0.437)\n",
      "   Puppies require training and socialization from an early age\n",
      "\n",
      "3. (Score: 0.413)\n",
      "   Cats are independent animals that make great companions\n",
      "\n",
      "================================================================================\n",
      "QUERY: How do I code in Python?\n",
      "================================================================================\n",
      "\n",
      "1. (Score: 0.554)\n",
      "   Python is a high-level programming language known for simplicity\n",
      "\n",
      "2. (Score: 0.148)\n",
      "   Puppies require training and socialization from an early age\n",
      "\n",
      "3. (Score: 0.138)\n",
      "   JavaScript is used for web development and runs in browsers\n"
     ]
    }
   ],
   "source": [
    "# Test different queries\n",
    "queries = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Tell me about pet dogs\",\n",
    "    \"How do I code in Python?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    results = search(query, documents, doc_embeddings, top_k=3)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. (Score: {score:.3f})\")\n",
    "        print(f\"   {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ad047",
   "metadata": {},
   "source": [
    "Comparing Different models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b429889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 1fc49098-6c69-4314-b9c9-05dabde18039)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 83307f74-43b6-4a32-affc-5d5acba3dfa8)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 57537c91-bf9d-4688-b74d-cf382f9d2d74)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 99b930e2-6ed2-4609-9052-68aacbaa6248)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 05d08a82-90bd-449c-8fa9-bea71ac65dcf)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 16c7fc64-7990-409a-afda-8f5f20eedd13)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 01ddd21d-c73e-4bc6-a7b2-f3c0ed07b6da)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./config_sentence_transformers.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43EFC980>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 89d3f375-2abe-492f-9ff6-8c31b768a81c)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./config_sentence_transformers.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43EFD490>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 6e4b8aec-45cc-4a39-9cf3-a1845aad57c5)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./config_sentence_transformers.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43EFE030>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: bc738adb-1960-49f8-aca3-29756da676ec)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./config_sentence_transformers.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43EFC9B0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 80102255-4844-498c-8ba7-b1b2a27969bc)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./config_sentence_transformers.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCE720>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: fb019d3f-80ad-440c-8edd-f61529b0ae6a)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./config_sentence_transformers.json\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCE5A0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 5e6d73be-fc96-47d1-b046-689d8c140173)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./config_sentence_transformers.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCFCB0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 6d9df29a-5391-4afe-b0b2-24b63cd4521a)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./config_sentence_transformers.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCF1D0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 744102cc-9dba-4bc9-bd86-48c8702f9c05)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./config_sentence_transformers.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCD610>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 543d6208-cc78-4987-9bfb-69f84a067b18)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./config_sentence_transformers.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F45298140>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: ea0996bb-2497-4cab-9fbd-ccfd9321fb0c)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./config_sentence_transformers.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43EFC860>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 3df66815-5747-4340-9685-fe69b6aa1cff)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./config_sentence_transformers.json\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43EFCF50>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: d7c606a3-631c-4210-acc0-a56fc24d5772)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./README.md\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCD640>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: f78c98e7-d703-4e92-8186-e741f40703e6)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./README.md\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCE4B0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 07c7a8c9-e597-44ea-b3a5-47b87f626b54)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./README.md\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCE330>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 2a900cf7-d956-40d2-8ff4-39b3b691305d)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./README.md\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCD010>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 4e7b2b2b-37b2-4943-b8bc-4c634da6ecbe)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./README.md\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DB42C0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 6bff4a69-5ef5-4514-81f4-4b57dd43a4c6)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./README.md\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DB6AE0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 5a96c1a8-e3e3-4d86-b530-478f633d792c)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCE720>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 1a33c960-fe4c-4907-883b-19f7965fcfc1)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCE030>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: ede2f1f1-f304-4ce7-948f-ec3440cb6c3e)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43EFC110>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: f2d806a3-1dd3-4012-98ba-f72540398aa3)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43EFCA10>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 73085589-3bf5-4bc3-ab69-77b0039e5a7e)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43EFF7A0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: d6cf522e-056a-4806-a9f8-2e268a4c3548)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DB6C00>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 206a2c73-113d-42b4-82dd-f5ea61bc6f30)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DB7FB0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 0e32e6de-8034-4ad1-9406-0be44e202cf0)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DB61B0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 70cf79fd-6a77-4b79-8174-b0a6e4ef996a)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DB6ED0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: c1b511d6-57cc-4daf-a3bb-234286ecc007)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DB6030>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 808f9f3a-ceb5-464f-99d4-ade8e949e8d0)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DB68D0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 919a7162-eed3-43b1-a30e-480a44e42ce9)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCFBC0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 617e7657-efa4-440f-b09c-a9e1dc08023d)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCC050>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 7a22446c-cd14-4c9e-8e4b-6375d39f2497)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DB5190>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: eee2bce5-985b-4f28-bdd4-2226086510d9)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DB7DD0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 3087b138-08f5-4306-a24c-3372e922474d)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DB47D0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 234abfff-ef3b-4469-90ac-9f3dc2bf93f7)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DB7D10>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 1fe48c55-d06f-47ca-be9d-4221fb438221)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F42AEA030>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 69a6be95-899e-4356-b571-a04f2e65e0cd)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F42AE8F80>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 48095d38-41a9-42f6-8caf-07654d5b61db)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F42AEA2A0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 286cb0e1-abef-4111-856a-77e16bb0e964)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCD850>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: ef227532-0476-4ef3-a14d-9df11e6007ce)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCE6F0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 7a424c2c-561b-4f63-80a8-e0b10655cfdc)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DB7770>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: ea9322c7-e5ea-4ea7-ab71-4488d9176788)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F42A3B350>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 4d86fc2c-d807-4ae2-a513-e6b28321003c)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43EFF770>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 0cb1dd77-5aa0-4800-9785-c8b0ef2d9605)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F42AE5FD0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 87db1702-d3dc-473e-87f6-bc9255f44624)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F42AE6090>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 082f223e-2f12-41a7-bcba-c6606ce8b849)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCD010>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 5548b934-19cf-432e-87e1-f0ffc61b537d)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43EFEFF0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 0a437e2c-64d4-4944-a2b2-8b75ec35e397)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F4535BBF0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 224e2393-e691-4a48-902b-6b449e3c00d2)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F4535B710>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 2fc7f49e-a63a-4554-99a1-e6d3646eff71)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F4535B110>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 0ea0cd30-e8bf-48a6-899d-43c6ff8a7b8a)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F4535AC00>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: ab89bcda-c6b9-405f-99da-c6a28ace284d)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F4535A5D0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 204a2497-0208-45fa-a3ac-0567cf1a7e85)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F4535A330>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 9685a659-3397-43a4-947d-dcb63ce549ca)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-mpnet-base-v2/resolve/main/modules.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F4535BE00>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 2059087a-8ca8-42b8-b0f1-5cdf2af69df6)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-mpnet-base-v2/resolve/main/modules.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F45359E50>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 0647a63a-b241-4fc5-be54-e13d632536f9)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/./modules.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-mpnet-base-v2/resolve/main/modules.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F45359940>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 79a78e4e-73b9-430a-93a7-1432be976c44)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/./modules.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-mpnet-base-v2/resolve/main/modules.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F45359430>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: aca7516d-ecf0-4752-8a40-ea5c9698efa8)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/./modules.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-mpnet-base-v2/resolve/main/modules.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F45359010>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 33d2abdf-5096-4889-8992-581d24314057)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/./modules.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-mpnet-base-v2/resolve/main/modules.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F45358B00>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 30392d91-fa80-4e76-a633-848f9bd0f70e)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/./modules.json\n",
      "No sentence-transformers model found with name sentence-transformers/all-mpnet-base-v2. Creating a new one with mean pooling.\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-mpnet-base-v2/resolve/main/adapter_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F42AE8650>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: f3d9078f-eacf-473b-9295-671c4c9f0ada)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/adapter_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-mpnet-base-v2/resolve/main/adapter_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCF200>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 7124eb2d-20d3-4a59-a358-551476b6d18f)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/adapter_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-mpnet-base-v2/resolve/main/adapter_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F43DCF680>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 3ed4a055-c3c5-4f7a-8449-484b77e56854)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/adapter_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-mpnet-base-v2/resolve/main/adapter_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F45298BF0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 59b456f7-939a-4ed6-9fa4-02e5263952c1)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/adapter_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-mpnet-base-v2/resolve/main/adapter_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F4535BB00>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 57b8eb93-660a-44fe-bdf4-a779ce18667f)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/adapter_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-mpnet-base-v2/resolve/main/adapter_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F45358FB0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 5b1006e9-b71e-4474-86e9-f64fb2bdc8db)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/adapter_config.json\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-mpnet-base-v2/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F4535A000>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 08fb4625-152e-4b5e-8fa4-5de69e6fafdd)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-mpnet-base-v2/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F4535B6E0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 322f5e92-e21b-4b60-af23-9db45f05b947)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-mpnet-base-v2/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F4535A540>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: de6bde13-7c40-4298-92d0-096b9262b7cf)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-mpnet-base-v2/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016F452980B0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: ebf78864-0719-4bf9-bec8-0b9a205f9f0b)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading models...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m model_small = SentenceTransformer(\u001b[33m'\u001b[39m\u001b[33mall-MiniLM-L6-v2\u001b[39m\u001b[33m'\u001b[39m)      \u001b[38;5;66;03m# 384 dimensions\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model_large = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mall-mpnet-base-v2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m     \u001b[38;5;66;03m# 768 dimensions\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Both models loaded!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSmall model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_small.get_sentence_embedding_dimension()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dimensions\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:339\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    327\u001b[39m         modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28mself\u001b[39m._load_sbert_model(\n\u001b[32m    328\u001b[39m             model_name_or_path,\n\u001b[32m    329\u001b[39m             token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    336\u001b[39m             config_kwargs=config_kwargs,\n\u001b[32m    337\u001b[39m         )\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m         modules = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_auto_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhas_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modules, OrderedDict):\n\u001b[32m    353\u001b[39m     modules = OrderedDict([(\u001b[38;5;28mstr\u001b[39m(idx), module) \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:2112\u001b[39m, in \u001b[36mSentenceTransformer._load_auto_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs, has_modules)\u001b[39m\n\u001b[32m   2109\u001b[39m tokenizer_kwargs = shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {**shared_kwargs, **tokenizer_kwargs}\n\u001b[32m   2110\u001b[39m config_kwargs = shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m config_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {**shared_kwargs, **config_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m2112\u001b[39m transformer_model = \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2120\u001b[39m pooling_model = Pooling(transformer_model.get_word_embedding_dimension(), \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:87\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     85\u001b[39m     config_args = {}\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m config, is_peft_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m._load_model(model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Get the signature of the auto_model's forward method to pass only the expected arguments from `features`,\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# plus some common values like \"input_ids\", \"attention_mask\", etc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:162\u001b[39m, in \u001b[36mTransformer._load_config\u001b[39m\u001b[34m(self, model_name_or_path, cache_dir, backend, config_args)\u001b[39m\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PeftConfig.from_pretrained(model_name_or_path, **config_args, cache_dir=cache_dir), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1332\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m trust_remote_code = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1330\u001b[39m code_revision = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1332\u001b[39m config_dict, unused_kwargs = \u001b[43mPretrainedConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1333\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1334\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\transformers\\configuration_utils.py:662\u001b[39m, in \u001b[36mPretrainedConfig.get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    660\u001b[39m original_kwargs = copy.deepcopy(kwargs)\n\u001b[32m    661\u001b[39m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    664\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\transformers\\configuration_utils.py:721\u001b[39m, in \u001b[36mPretrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    717\u001b[39m configuration_file = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33m_configuration_file\u001b[39m\u001b[33m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    720\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m721\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\transformers\\utils\\hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\transformers\\utils\\hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    494\u001b[39m         snapshot_download(\n\u001b[32m    495\u001b[39m             path_or_repo_id,\n\u001b[32m    496\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    505\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    506\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1007\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    988\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    989\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1005\u001b[39m     )\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1070\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1066\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[32m   1068\u001b[39m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[32m   1069\u001b[39m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) = \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1093\u001b[39m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[32m   1094\u001b[39m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1096\u001b[39m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1543\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1541\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1542\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1543\u001b[39m         metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1546\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[32m   1547\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1548\u001b[39m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1460\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1457\u001b[39m hf_headers[\u001b[33m\"\u001b[39m\u001b[33mAccept-Encoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33midentity\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[32m   1459\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1460\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1469\u001b[39m hf_raise_for_status(r)\n\u001b[32m   1471\u001b[39m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:283\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    291\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m300\u001b[39m <= response.status_code <= \u001b[32m399\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:306\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m    305\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m response = \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m hf_raise_for_status(response)\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:329\u001b[39m, in \u001b[36mhttp_backoff\u001b[39m\u001b[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# Sleep for X seconds\u001b[39;00m\n\u001b[32m    328\u001b[39m logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRetrying in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms [Retry \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_tries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m].\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# Update sleep time for next retry\u001b[39;00m\n\u001b[32m    332\u001b[39m sleep_time = \u001b[38;5;28mmin\u001b[39m(max_wait_time, sleep_time * \u001b[32m2\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# # Load two different models for comparison\n",
    "# print(\"Loading models...\\n\")\n",
    "\n",
    "# model_small = SentenceTransformer('all-MiniLM-L6-v2')      # 384 dimensions\n",
    "# model_large = SentenceTransformer('all-mpnet-base-v2')     # 768 dimensions\n",
    "\n",
    "# print(\"✅ Both models loaded!\")\n",
    "# print(f\"Small model: {model_small.get_sentence_embedding_dimension()} dimensions\")\n",
    "# print(f\"Large model: {model_large.get_sentence_embedding_dimension()} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3dcfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing model performance:\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_small' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mComparing model performance:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text1, text2 \u001b[38;5;129;01min\u001b[39;00m test_pairs:\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Small model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     emb1_small = \u001b[43mmodel_small\u001b[49m.encode([text1, text2])\n\u001b[32m     12\u001b[39m     sim_small = cosine_similarity(emb1_small[\u001b[32m0\u001b[39m], emb1_small[\u001b[32m1\u001b[39m])\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Large model  \u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'model_small' is not defined"
     ]
    }
   ],
   "source": [
    "# # Compare on a similarity task\n",
    "# test_pairs = [\n",
    "#     (\"The dog is running\", \"A canine is jogging\"),           # Similar\n",
    "#     (\"I love pizza\", \"Pizza is delicious\"),                  # Related\n",
    "#     (\"Python programming\", \"Cooking pasta\")                  # Unrelated\n",
    "# ]\n",
    "\n",
    "# print(\"Comparing model performance:\\n\")\n",
    "# for text1, text2 in test_pairs:\n",
    "#     # Small model\n",
    "#     emb1_small = model_small.encode([text1, text2])\n",
    "#     sim_small = cosine_similarity(emb1_small[0], emb1_small[1])\n",
    "    \n",
    "#     # Large model  \n",
    "#     emb1_large = model_large.encode([text1, text2])\n",
    "#     sim_large = cosine_similarity(emb1_large[0], emb1_large[1])\n",
    "    \n",
    "#     print(f\"Pair: '{text1}' vs '{text2}'\")\n",
    "#     print(f\"  Small model: {sim_small:.3f}\")\n",
    "#     print(f\"  Large model: {sim_large:.3f}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85675483",
   "metadata": {},
   "source": [
    "# make sue you complete module 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df50915",
   "metadata": {},
   "source": [
    "## Module 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e1c1fc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m \n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np \n",
    "from sentence_transformers import SentenceTransformer\n",
    "print(f\"FAISS version: {faiss.__version__}\")\n",
    "print(\"✅ Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8009cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(documents)\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Each embedding has {embeddings.shape[1]} dimensions\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding dimension\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# Create FAISS index (IndexFlatL2 = exact search with L2 distance)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to index\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"✅ FAISS index created!\")\n",
    "print(f\"Total vectors in index: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"Python is a versatile programming language used for web development and data science.\",\n",
    "    \"Machine learning models require large amounts of training data to perform well.\",\n",
    "    \"Neural networks are inspired by the structure of the human brain.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Deep learning is a subset of machine learning using multi-layered neural networks.\",\n",
    "    \"Data visualization helps communicate insights from complex datasets.\",\n",
    "    \"Cloud computing provides on-demand access to computing resources.\",\n",
    "    \"Cybersecurity protects systems and networks from digital attacks.\",\n",
    "    \"Blockchain technology enables secure, decentralized transactions.\",\n",
    "    \"Quantum computing uses quantum mechanics to solve complex problems.\"\n",
    "]\n",
    "\n",
    "print(f\"Total documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc5a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "query = \"What is artificial intelligence and machine learning?\"\n",
    "\n",
    "# Embed query\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "# Search: find top 3 most similar vectors\n",
    "k = 3\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Top {k} results:\\n\")\n",
    "\n",
    "for i, (idx, distance) in enumerate(zip(indices[0], distances[0]), 1):\n",
    "    print(f\"{i}. (Distance: {distance:.4f})\")\n",
    "    print(f\"   {documents[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ba709",
   "metadata": {},
   "source": [
    "## Using Cosine Similarity with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca9d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize embeddings for cosine similarity\n",
    "embddings_normalized = embeddings/np.linalg.norm(embeddings,axis=1,keepdims=True)\n",
    "\n",
    "#Create index with inner product(equivalent to cosine for normalized vectors)\n",
    "index_cosine=faiss.IndexFlatIP(dimension)\n",
    "index_cosine.add(embddings_normalized)\n",
    "\n",
    "#search with normalized query\n",
    "# Search with normalized query\n",
    "query_embedding_normalized = query_embedding / np.linalg.norm(query_embedding)\n",
    "scores, indices = index_cosine.search(query_embedding_normalized, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Top {k} results with cosine similarity:\\n\")\n",
    "\n",
    "for i, (idx, score) in enumerate(zip(indices[0], scores[0]), 1):\n",
    "    print(f\"{i}. (Similarity: {score:.4f})\")\n",
    "    print(f\"   {documents[idx]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211b845",
   "metadata": {},
   "source": [
    "### Saving and Loading faiss index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save index to disk\n",
    "\n",
    "faiss.write_index(index_cosine,\"my_faiss_index.bin\")\n",
    "import pickle\n",
    "with open(\"documents.pkl\",\"wb\") as f:\n",
    "    pickle.dump(documents,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e38d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load index from disk\n",
    "loaded_index = faiss.read_index(\"my_faiss_index.bin\")\n",
    "print(f\"✅ Index loaded: {loaded_index.ntotal} vectors\")\n",
    "\n",
    "# Load documents\n",
    "with open(\"documents.pkl\", \"rb\") as f:\n",
    "    loaded_documents = pickle.load(f)\n",
    "print(f\"✅ Documents loaded: {len(loaded_documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48dd0a",
   "metadata": {},
   "source": [
    "# Chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3366ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chroma version 1.3.5\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "print(\"chroma version\",chromadb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a0a95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Collection created: my_documents\n",
      "Current count: 0 documents\n",
      "📁 Data persisted to: ./chroma_db/\n"
     ]
    }
   ],
   "source": [
    "#create chroma client(persistent client)\n",
    "client= chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "#create or get collection\n",
    "\n",
    "collection= client.get_or_create_collection(name=\"my_documents\",metadata={\"description\":\"Sample documents\"})\n",
    "\n",
    "print(f\"✅ Collection created: {collection.name}\")\n",
    "print(f\"Current count: {collection.count()} documents\")\n",
    "print(f\"📁 Data persisted to: ./chroma_db/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1d91b9",
   "metadata": {},
   "source": [
    "## Add document to chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9128fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents with metadata\n",
    "documents = [\n",
    "    \"Python is a versatile programming language used for web development and data science.\",\n",
    "    \"Machine learning models require large amounts of training data to perform well.\",\n",
    "    \"Neural networks are inspired by the structure of the human brain.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Deep learning is a subset of machine learning using multi-layered neural networks.\"\n",
    "]\n",
    "\n",
    "# Metadata for each document\n",
    "metadatas = [\n",
    "    {\"category\": \"programming\", \"topic\": \"python\"},\n",
    "    {\"category\": \"AI\", \"topic\": \"machine learning\"},\n",
    "    {\"category\": \"AI\", \"topic\": \"neural networks\"},\n",
    "    {\"category\": \"AI\", \"topic\": \"NLP\"},\n",
    "    {\"category\": \"AI\", \"topic\": \"deep learning\"}\n",
    "]\n",
    "\n",
    "## its looking like chromma takes the documents or chunks in form of a list each seprated sentence by a comma\n",
    "\n",
    "#chroma needs ids to add into collection\n",
    "\n",
    "#creATING ids fro each documents \n",
    "\n",
    "ids= [f\"doc_{i}\" for i in range(len(documents))]\n",
    "\n",
    "#adding to collection(chromma handles embeddings automatically)\n",
    "collection.add(documents=documents,metadatas=metadatas,ids=ids)\n",
    "\n",
    "print(f\"✅ Added {len(documents)} documents to collection\")\n",
    "print(f\"Total documents: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164fcce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query the collection\n",
    "\n",
    "result= collection.query(query_texts=[\"what is artificial intelligence\"],n_results=3)\n",
    "\n",
    "print(\"Query: What is artificial intelligence?\\n\")\n",
    "print(\"Top 3 results:\\n\")\n",
    "\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0],\n",
    "    results['distances'][0]\n",
    "), 1):\n",
    "    print(f\"{i}. (Distance: {distance:.4f})\")\n",
    "    print(f\"   Document: {doc}\")\n",
    "    print(f\"   Metadata: {metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "addd212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "#use sentence-transformer embedding function \n",
    "sentence_trans_embeding_func= embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "colection_custom= client.get_or_create_collection(name=\"custom_embeddingscollection\",embedding_function=sentence_trans_embeding_func)\n",
    "\n",
    "\n",
    "#adding to the ollection \n",
    "colection_custom.add(documents=documents,metadatas=metadatas,ids=ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dc05f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colection_custom.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97382cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results= colection_custom.query(query_texts=[\"What is artificial intelligence?\"],n_results=3,include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ffd3659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['doc_4', 'doc_3', 'doc_2']],\n",
       " 'embeddings': [array([[-0.08768701, -0.026981  ,  0.06650561, ...,  0.0354312 ,\n",
       "          -0.00644909,  0.01132433],\n",
       "         [ 0.01383317,  0.01758453,  0.09789699, ...,  0.08880782,\n",
       "           0.04175101, -0.056559  ],\n",
       "         [-0.06560455, -0.08392676,  0.05001441, ...,  0.16255856,\n",
       "           0.05264854, -0.06086662]], shape=(3, 384))],\n",
       " 'documents': [['Deep learning is a subset of machine learning using multi-layered neural networks.',\n",
       "   'Natural language processing enables computers to understand human language.',\n",
       "   'Neural networks are inspired by the structure of the human brain.']],\n",
       " 'uris': None,\n",
       " 'included': ['embeddings', 'documents', 'metadatas', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'topic': 'deep learning', 'category': 'AI'},\n",
       "   {'category': 'AI', 'topic': 'NLP'},\n",
       "   {'category': 'AI', 'topic': 'neural networks'}]],\n",
       " 'distances': [[0.5752362012863159, 0.6204118132591248, 0.6279884576797485]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a56b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update a document\n",
    "collection.update(\n",
    "    ids=[\"doc_0\"],\n",
    "    documents=[\"Python is an amazing programming language for AI and data science!\"],\n",
    "    metadatas=[{\"category\": \"programming\", \"topic\": \"python\", \"updated\": True}]\n",
    ")\n",
    "print(\"✅ Document updated\")\n",
    "\n",
    "# Delete a document\n",
    "# collection.delete(ids=[\"doc_4\"])\n",
    "# print(\"✅ Document deleted\")\n",
    "\n",
    "print(f\"\\nTotal documents after update: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1faff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea24ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# class RAGretriever:\n",
    "#     def __init__(self,collection_name='rag_collection',persist_dir='./rag_db'):\n",
    "#         #create chroma client(using persistentclient for chromadb)\n",
    "#         self.client= chromadb.PersistentClient(path=persist_dir)\n",
    "\n",
    "#         #create collectiom with sentence tramnsformer \n",
    "#         embedding_fn= embedding_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8901747",
   "metadata": {},
   "source": [
    "# module 5 (Prompt Augmentation & Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036dbd15",
   "metadata": {},
   "source": [
    "This module is all about prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c849ab",
   "metadata": {},
   "source": [
    "# Module 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6f7858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"openai_key\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"⚠️ Warning: OPENAI_API_KEY not found. Set it in .env file.\")\n",
    "else:\n",
    "    print(\"✅ API key loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dc88ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents for all examples\n",
    "documents = [\n",
    "    \"Python is a high-level programming language known for readability and simplicity.\",\n",
    "    \"Machine learning is a subset of AI that enables systems to learn from data.\",\n",
    "    \"RAG combines retrieval and generation to provide accurate, grounded responses.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553202ef",
   "metadata": {},
   "source": [
    "### Using Langchain implementaton with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b6a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='RAG combines retrieval and generation to provide accurate, grounded responses.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 175, 'total_tokens': 188, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_ee69c2ef48', 'id': 'chatcmpl-CnVlJTvzF5ZNA7Fb6XiWXfqyGX3a5', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b28ce-c8e9-7340-9290-8e56dba06c9c-0' usage_metadata={'input_tokens': 175, 'output_tokens': 13, 'total_tokens': 188, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.docstore.document import Document\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel,RunnablePassthrough\n",
    "\n",
    "\n",
    "# convert documents\n",
    "loc_docs=[Document(page_content=doc) for doc in documents]\n",
    "\n",
    "#vector store\n",
    "embeddings= OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "vectorstore=FAISS.from_documents(loc_docs,embeddings)\n",
    "retriever= vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "#llm \n",
    "llm=ChatOpenAI(model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    openai_api_key=openai_api_key)\n",
    "\n",
    "\n",
    "#Prompt\n",
    "prompt= ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert assistant. Use ONLY the retrieved context.\"),\n",
    "    (\"human\", \"{question}\\n\\nContext:\\n{context}\")\n",
    "])\n",
    "\n",
    "#Build RAG pipeline\n",
    "rag_chain = (RunnableParallel(context=retriever,question=RunnablePassthrough())\n",
    "             |prompt\n",
    "             |llm\n",
    "             )\n",
    "\n",
    "# Query\n",
    "response = rag_chain.invoke(\"what is RAG\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d17ba0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG combines retrieval and generation to provide accurate, grounded responses.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13459d8c",
   "metadata": {},
   "source": [
    "## Approach 3: LlamaIndex IMplemention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d0e89a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://developers.llamaindex.ai/python/framework/module_guides/models/embeddings/\n******",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\llama_index\\core\\embeddings\\utils.py:59\u001b[39m, in \u001b[36mresolve_embed_model\u001b[39m\u001b[34m(embed_model, callback_manager)\u001b[39m\n\u001b[32m     58\u001b[39m     embed_model = OpenAIEmbedding()\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[43mvalidate_openai_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\llama_index\\embeddings\\openai\\utils.py:104\u001b[39m, in \u001b[36mvalidate_openai_api_key\u001b[39m\u001b[34m(api_key)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[31mValueError\u001b[39m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Create documents and Index \u001b[39;00m\n\u001b[32m     12\u001b[39m llama_docs =[Document(text=doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m index= \u001b[43mVectorStoreIndex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllama_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#Query\u001b[39;00m\n\u001b[32m     17\u001b[39m query_engine= index.as_query_engine()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\llama_index\\core\\indices\\base.py:122\u001b[39m, in \u001b[36mBaseIndex.from_documents\u001b[39m\u001b[34m(cls, documents, storage_context, show_progress, callback_manager, transformations, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m     docstore.set_document_hash(doc.id_, doc.hash)\n\u001b[32m    115\u001b[39m nodes = run_transformations(\n\u001b[32m    116\u001b[39m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    117\u001b[39m     transformations,\n\u001b[32m    118\u001b[39m     show_progress=show_progress,\n\u001b[32m    119\u001b[39m     **kwargs,\n\u001b[32m    120\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:71\u001b[39m, in \u001b[36mVectorStoreIndex.__init__\u001b[39m\u001b[34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28mself\u001b[39m._use_async = use_async\n\u001b[32m     69\u001b[39m \u001b[38;5;28mself\u001b[39m._store_nodes_override = store_nodes_override\n\u001b[32m     70\u001b[39m \u001b[38;5;28mself\u001b[39m._embed_model = resolve_embed_model(\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     embed_model \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSettings\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_model\u001b[49m, callback_manager=callback_manager\n\u001b[32m     72\u001b[39m )\n\u001b[32m     74\u001b[39m \u001b[38;5;28mself\u001b[39m._insert_batch_size = insert_batch_size\n\u001b[32m     75\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     76\u001b[39m     nodes=nodes,\n\u001b[32m     77\u001b[39m     index_struct=index_struct,\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m     **kwargs,\n\u001b[32m     84\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\llama_index\\core\\settings.py:64\u001b[39m, in \u001b[36m_Settings.embed_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the embedding model.\"\"\"\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embed_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28mself\u001b[39m._embed_model = \u001b[43mresolve_embed_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdefault\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mself\u001b[39m._embed_model.callback_manager = \u001b[38;5;28mself\u001b[39m._callback_manager\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\llama_index\\core\\embeddings\\utils.py:66\u001b[39m, in \u001b[36mresolve_embed_model\u001b[39m\u001b[34m(embed_model, callback_manager)\u001b[39m\n\u001b[32m     61\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     62\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`llama-index-embeddings-openai` package not found, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     63\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mplease run `pip install llama-index-embeddings-openai`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     64\u001b[39m         )\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     67\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCould not load OpenAI embedding model. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mConsider using embed_model=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mlocal\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mVisit our documentation for more embedding options: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttps://developers.llamaindex.ai/python/framework/module_guides/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodels/embeddings/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m******\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m         )\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# for image multi-modal embeddings\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m embed_model.startswith(\u001b[33m\"\u001b[39m\u001b[33mclip\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: \n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://developers.llamaindex.ai/python/framework/module_guides/models/embeddings/\n******"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document, VectorStoreIndex,settings\n",
    "from llama_index.llms.openai import OpenAI as LlamaOpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "\n",
    "#configure llamaindex\n",
    "\n",
    "settings.llm= LlamaOpenAI(model=\"gpt-3.5-turbo\", temperature=0, api_key=openai_api_key)\n",
    "settings.embed_model= OpenAIEmbedding(api_key=openai_api_key)\n",
    "\n",
    "# Create documents and Index \n",
    "llama_docs =[Document(text=doc) for doc in documents]\n",
    "index= VectorStoreIndex.from_documents(llama_docs)\n",
    "\n",
    "\n",
    "#Query\n",
    "query_engine= index.as_query_engine()\n",
    "response= query_engine.query(\"what is RAG?\")\n",
    "\n",
    "print(\"LlamaIndex Answer:\")\n",
    "print(response)\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef22a760",
   "metadata": {},
   "source": [
    "# Module 8 RAG with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73d897d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 documents\n",
      "First document: LangChain is a framework for developing applications powered by language models.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create documents directly (you can also load from files using loaders)\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain is a framework for developing applications powered by language models.\"),\n",
    "    Document(page_content=\"RAG combines retrieval with generation to provide accurate, grounded responses.\"),\n",
    "    Document(page_content=\"Vector stores enable efficient similarity search over embedded documents.\"),\n",
    "    Document(page_content=\"FAISS is a library for efficient similarity search developed by Meta AI.\"),\n",
    "    Document(page_content=\"Text splitters chunk documents into smaller pieces for better retrieval.\")\n",
    "]\n",
    "\n",
    "print(f\"Created {len(documents)} documents\")\n",
    "print(f\"First document: {documents[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4642a7ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      3\u001b[39m text_splitter=RecursiveCharacterTextSplitter(chunk_size=\u001b[32m100\u001b[39m,\n\u001b[32m      4\u001b[39m                                              chunk_overlap=\u001b[32m20\u001b[39m,\n\u001b[32m      5\u001b[39m                                              length_function=\u001b[38;5;28mlen\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# split documents\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m chunks= text_splitter.split_documents(\u001b[43mdocuments\u001b[49m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSplit \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks[:\u001b[32m3\u001b[39m]):\n",
      "\u001b[31mNameError\u001b[39m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=100,\n",
    "                                             chunk_overlap=20,\n",
    "                                             length_function=len)\n",
    "\n",
    "# split documents\n",
    "chunks= text_splitter.split_documents(documents)\n",
    "print(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}: {chunk.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb734d9",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "992defd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 1536\n",
      "First 5 values: [0.0006281227106228471, 0.02569717727601528, 0.007161187008023262, 0.03336399793624878, -0.031968604773283005]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "# Test embedding\n",
    "test_embedding = embeddings.embed_query(\"What is RAG?\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "print(f\"First 5 values: {test_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7118cff3",
   "metadata": {},
   "source": [
    "## vectorsctore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9efc52d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#CREATE VECTOR STORE FROM DOCUMENTS \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m vectorstore= FAISS.from_documents(\u001b[43mchunks\u001b[49m,embeddings)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Vector store created with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Test similarity search\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "#CREATE VECTOR STORE FROM DOCUMENTS \n",
    "vectorstore= FAISS.from_documents(chunks,embeddings)\n",
    "print(f\"✅ Vector store created with {len(chunks)} chunks\")\n",
    "\n",
    "# Test similarity search\n",
    "query = \"What is FAISS?\"\n",
    "results=vectorstore.similarity_search(query,k=2)\n",
    "\n",
    "print(f\"\\nQuery: {query}\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}: {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5436259",
   "metadata": {},
   "source": [
    "# Part2 Building RAG with LCEL (langchain expression LAnguage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0823ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough ,RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "#create llm\n",
    "\n",
    "llm= ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "#create retriever\n",
    "retriever =vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "#create prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer using ONLY the provided context.\"),\n",
    "    (\"human\", \"{question}\\n\\nContext:\\n{context}\")])\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain =(RunnableParallel(context=retriever | format_docs, question=RunnablePassthrough())\n",
    "            |prompt\n",
    "            |llm\n",
    "            |StrOutputParser)\n",
    "\n",
    "print(\"✅ RAG chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the chain\n",
    "response = rag_chain.invoke(\"What is LangChain?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef3ce83",
   "metadata": {},
   "source": [
    "## Simple Conversational RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "chat_store ={}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in chat_store:\n",
    "        chat_store[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_store[session_id]\n",
    "\n",
    "\n",
    "#create conversational prompt\n",
    "conv_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer using the context provided.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "#build base chain\n",
    "con_chain_base=(RunnableParallel(\n",
    "    context=lambda x: format_docs(retriever.invoke(x[\"question\"])),\n",
    "    question=lambda x: x[\"question\"],\n",
    "    chat_history=lambda x: x.get(\"chat_history\", [])\n",
    ")\n",
    "|conv_prompt\n",
    "|llm\n",
    "|StrOutputParser())\n",
    "\n",
    "\n",
    "#wrap with message history\n",
    "conv_chain= RunnableWithMessageHistory(con_chain_base,get_session_history,input_messages_key=\"question\",history_messages_key=\"chat_history\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad23d39",
   "metadata": {},
   "source": [
    "### Production_RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5c19fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\Working_with_LLMS\\llmvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "608053a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API key loaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env file\n",
    "# Create a .env file in this directory with: OPENAI_API_KEY=your-key-here\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"openai_key\")\n",
    "\n",
    "if api_key:\n",
    "    print(\"✅ API key loaded!\")\n",
    "else:\n",
    "    print(\"⚠️  Warning: OPENAI_API_KEY not found in .env file\")\n",
    "    print(\"Create a .env file with: OPENAI_API_KEY=your-key-here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b54378",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader= DirectoryLoader('documents/',glob=\"**/*.txt\",loader_cls=TextLoader)\n",
    "documents=loader.load()\n",
    "print(f\"✅ Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a8c0b",
   "metadata": {},
   "source": [
    "#### chunk documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b2a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter= RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "chunks= text_splitter.split_documents(documents)\n",
    "print(f\"✅ Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a29137",
   "metadata": {},
   "source": [
    "### Create Embeddings \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",openai_api_key=api_key)\n",
    "\n",
    "test_embedding= embeddings.embed_query(\"what is RAG\")\n",
    "print(f\"✅ Embedding model loaded!\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc732f",
   "metadata": {},
   "source": [
    "## Create Vextor Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6800a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Create vector store...\")\n",
    "vectorstore= Chroma.from_documents(documents=chunks,embedding=embeddings,persist_directory=\"./chroma_db\")\n",
    "\n",
    "print(f\"Vector store created with {len(chunks)} chunks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f59f32",
   "metadata": {},
   "source": [
    "## Step 7b: OR Load Existing Vector Store (Skip Step 4-7 if using this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing vector store (uncomment to use)\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "print(f\"✅ Loaded existing vector store!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdd8f89",
   "metadata": {},
   "source": [
    "## Step 7c: Smart Approach - Load if Exists, Create if Not\n",
    "\n",
    "**Best practice**: Check if the vector store already exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart approach - load if exists, create if not (uncomment to use)\n",
    "# import os\n",
    "# \n",
    "# if os.path.exists(\"./chroma_db\"):\n",
    "#     print(\"Loading existing vector store...\")\n",
    "#     vectorstore = Chroma(\n",
    "#         persist_directory=\"./chroma_db\",\n",
    "#         embedding_function=embeddings\n",
    "#     )\n",
    "#     print(\"✅ Loaded existing vector store!\")\n",
    "# else:\n",
    "#     print(\"Creating new vector store...\")\n",
    "#     vectorstore = Chroma.from_documents(\n",
    "#         documents=chunks,\n",
    "#         embedding=embeddings,\n",
    "#         persist_directory=\"./chroma_db\"\n",
    "#     )\n",
    "#     print(f\"✅ Vector store created with {len(chunks)} chunks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e8eff9",
   "metadata": {},
   "source": [
    "## Step 8: Set Up OpenAI LLM\n",
    "\n",
    "We'll use GPT-3.5-turbo for fast, cost-effective answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b9a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize OPenai llm \n",
    "llm= ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0,openai_api_key=api_key)\n",
    "\n",
    "\n",
    "print(\"✅ OpenAI LLM initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce64d5",
   "metadata": {},
   "source": [
    "## Step 9: Test Retrieval Only\n",
    "\n",
    "Before we do full generation, let's see what documents we retrieve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc7a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever= vectorstore.as_retriever(search_kwargs={\"k\":3})\n",
    "\n",
    "#test retreival \n",
    "query= \"What is machine learning?\"\n",
    "retrieved_docs= retriever.invoke(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\\n\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"{i}. {doc.page_content[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d2351",
   "metadata": {},
   "source": [
    "## Step 10: Create the RAG Prompt Template\n",
    "\n",
    "This prompt tells the LLM how to use the retrieved context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"You are a helpful assistant answering questions based on the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:{question}\n",
    "Answer the question based on the context above. if you cannot answer based on the context,say so.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt= ChatPromptTemplate.from_template(template)\n",
    "print(\"Prompt template created!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f3aba",
   "metadata": {},
   "source": [
    "## Build the RAG Chain\n",
    "This combinesretrieval + generation into one pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db3229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f067509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✅ RAG chain built!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e0663",
   "metadata": {},
   "source": [
    "#  RAG CORE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4508968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from typing import List,Dict\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import DirectoryLoader,TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d32cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessors:\n",
    "    def __init__(self,chunk_size: int=500,chunk_overlap:int=50):\n",
    "        self.chunk_size= chunk_size\n",
    "        self.chunk_overlap=chunk_overlap\n",
    "        self.text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap,length_function=len)\n",
    "\n",
    "    def load_documents(self,documents_folder:str) -> List[Document]:\n",
    "        loader = DirectoryLoader(\n",
    "            documents_folder,\n",
    "            glob=\"**/*.txt\",\n",
    "            loader_cls=TextLoader\n",
    "        )\n",
    "        documents = loader.load()\n",
    "        print(f\"✅ Loaded {len(documents)} documents\")\n",
    "        return documents\n",
    "    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        chunks=self.text_splitter.split_documents(documents)\n",
    "        print(f\"✅ Created {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "    \n",
    "\n",
    "class VectorStoreManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        persist_directory: str = \"./chroma_db\"):\n",
    "        \n",
    "        load_dotenv()\n",
    "        self.embedding_model=embedding_model\n",
    "        self.persistent_directory= persist_directory\n",
    "        # Initialize embeddings\n",
    "        print(f\"Loading embedding model: {embedding_model}...\")\n",
    "        self.embeddings = OpenAIEmbeddings(\n",
    "            model=embedding_model\n",
    "        )\n",
    "        print(\"✅ Embedding model loaded!\")\n",
    "\n",
    "        self.vectorstore = None\n",
    "\n",
    "    def create_vectorestore(self,chunks: List[Document]):\n",
    "        \"\"\"\n",
    "        Create a vector store from document chunks.\n",
    "        \"\"\"\n",
    "        print(\"Creating vector store with embeddings...\")\n",
    "        self.vectorstore=Chroma.from_documents(documents=chunks,embedding=self.embeddings,persist_directory=self.persistent_directory)\n",
    "        print(f\"✅ Vector store created with {len(chunks)} chunks!\")\n",
    "\n",
    "    def load_vectorstore(self):\n",
    "        \"\"\"\n",
    "        Load an existing vector store from disk.\n",
    "        \"\"\"\n",
    "        print(\"Loading existing vector store...\")\n",
    "        self.vectorestore=Chroma(persist_directory=self.persistent_directory,embedding_function=self.embeddings)\n",
    "        print(\"✅ Vector store loaded!\")\n",
    "    def search(self, query: str, top_k: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        Search for relevant documents.\n",
    "        \"\"\"\n",
    "        if self.vectorestore is None:\n",
    "            raise ValueError(\"No vector store loaded. Create or load one first.\")\n",
    "        results_with_scores= self.vectorestore.similarity_search_with_score(query,k=top_k)\n",
    "        formatted_results = {\n",
    "            \"query\": query,\n",
    "            \"results\": []\n",
    "        }\n",
    "\n",
    "        for doc, score in results_with_scores:\n",
    "            formatted_results[\"results\"].append({\n",
    "                \"text\": doc.page_content,\n",
    "                \"metadata\": doc.metadata,\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "\n",
    "        return formatted_results\n",
    "    \n",
    "    def get_retriever(self, top_k: int = 3):\n",
    "        \"\"\"\n",
    "        Get a LangChain retriever for the vector store.\n",
    "        \"\"\"\n",
    "        if self.vectorstore is None:\n",
    "            raise ValueError(\"No vector store loaded. Create or load one first.\")\n",
    "\n",
    "        return self.vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get statistics about the vector store.\n",
    "        \"\"\"\n",
    "        if self.vectorstore is None:\n",
    "            return {\"error\": \"No vector store loaded\"}\n",
    "\n",
    "        collection = self.vectorstore._collection\n",
    "        return {\n",
    "            \"total_chunks\": collection.count(),\n",
    "            \"embedding_model\": self.embedding_model\n",
    "        }\n",
    "\n",
    "\n",
    "class RAGGenerator:\n",
    "    \"\"\"\n",
    "    Generates answers by combining retrieval with an LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vectorstore_manager: VectorStoreManager,\n",
    "        openai_model: str = \"gpt-3.5-turbo\",\n",
    "        temperature: float = 0.0,\n",
    "        top_k: int = 3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the RAG generator.\n",
    "        \"\"\"\n",
    "        load_dotenv()\n",
    "\n",
    "        self.vectorstore_manager = vectorstore_manager\n",
    "        self.openai_model = openai_model\n",
    "        self.temperature = temperature\n",
    "        self.top_k = top_k\n",
    "\n",
    "        # Initialize LLM\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=openai_model,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        print(f\"✅ OpenAI LLM initialized ({openai_model})!\")\n",
    "\n",
    "        # Build RAG chain\n",
    "        self._build_rag_chain()\n",
    "    def _build_rag_chain(self):\n",
    "        \"\"\"\n",
    "        Build the RAG chain (retrieval + generation pipeline).\n",
    "        \"\"\"\n",
    "        # Get retriever\n",
    "        retriever = self.vectorstore_manager.get_retriever(top_k=self.top_k)\n",
    "\n",
    "        # Create prompt template\n",
    "        template = \"\"\"You are a helpful assistant answering questions based on the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer the question based on the context above. If you cannot answer based on the context, say so.\n",
    "\n",
    "Answer:\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "        # Helper function to format documents\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "        # Build the RAG chain\n",
    "        self.rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "\n",
    "        )\n",
    "\n",
    "        print(\"✅ RAG chain built!\")\n",
    "    def query(self, question: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Ask a question and get an AI-generated answer.\n",
    "        \"\"\"\n",
    "        answer = self.rag_chain.invoke(question)\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        }\n",
    "# Convenience function to build a complete RAG system\n",
    "def build_rag_system(documents_folder: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Build a complete RAG system from documents.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BUILDING RAG SYSTEM\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    doc_processor=DocumentProcessors()\n",
    "    documents=doc_processor.load_documents(documents_folder)\n",
    "    chunks= doc_processor.chunk_documents(documents)\n",
    "\n",
    "    #Step 2\n",
    "    vectorstore_manager = VectorStoreManager()\n",
    "    vectorstore_manager.create_vectorstore(chunks)\n",
    "\n",
    "    # Step 3: Create RAG generator\n",
    "    rag_generator = RAGGenerator(vectorstore_manager)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"RAG SYSTEM READY!\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    return doc_processor, vectorstore_manager, rag_generator\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
